
\subsection{Small Boat Fleet and their Carbon Emissions}
Previous work related to estimating small-scale vessels without machine learning methods includes using top-down and bottom-up approaches, and statistical factors. 

Parker et al.~\cite{parker2018fuel} used a top-down approach to estimate the fishing sector emission in 2011 which reached about 179 Mt \ch{CO2e} representing 17.1\% of the total large fishing ship emissions in that year~\cite{smith2015third}. However, their work only distinguished between motorized and not motorized fishing vessels. Greer et al.~\cite{GREER2019103382} took a bottom-up approach classifying the fishing fleet in six different sizes, three of them below 25 m long. The findings show that the small fishing boat fleet in 2016 emitted 47 Mt \ch{CO2} about 22.7\% of the total fishing fleet. Ferrer et al.~\cite{ferrer2021mexican} used an activity based method using GPS, landing and fuel used data to estimate the fishing activity around Baja California Peninsula in Mexico. They found that just the small-scale fishing fleet produced 3.4 Mt of \ch{CO2e} in 2014 to put into context, Mexicoâ€™s national inventory for the shipping sector in 2014 was recorded at just 2.2 Mt \ch{CO2e}, clearly putting into perspective the role of this fleet segment on the national inventories~\cite{inecc2020inventario}.

Several authors have proposed the use of AIS to monitor the carbon emissions of the fleet as well~\cite{Traut2013MonitoringSE, Johansson2016ACM, Mabunda2014EstimatingCD, Hensel2020GreenSU, Han2016RealtimeIA}. Johansson et al.~\cite{Johansson2018ModelingOL} proposed a new model (FMI-BEAM) to describe the emissions of the leisure boat fleet in the Baltic Sea region with over 3000 dock locations, national small boat registry, AIS data and vessel survey results. However, the method cannot cover countries with no national registry for small boats. Besides, small boats are not just leisure boats. Ug{\'e} et al.~\cite{Ug2020EstimationOW} estimated global ship emissions with the help of data from the Automatic Identification System (AIS). They used more than three billion daily AIS data records to create an activity database that captured ship size and speed, meteorological and marine environmental conditions. However, this method is highly dependent on AIS data which does not capture the activity of the small boats.

Zhang et al. included unidentified vessels in the AIS-based vessel emission inventory~\cite{Zhang2019TheSO}. They developed an AIS-instrumented emissions inventory, including both identified and unidentified vessels. In particular, missing vessel parameters for unidentified vessels were estimated from a classification regression of vessels with similar vessel types and sizes in the AIS database. However, the authors do not discuss whether the regression model applies to vessels in most coastal areas. In addition, the authors do not discuss whether the vessel data in the AIS database is regionally diverse. Finally, if there is a diversity of vessels in the AIS database, the authors did not discuss whether this diversity would produce more significant errors in the predictions for small vessels in a single region (e.g. the Gulf of California, Mexico).

\subsection{Convolutional Neural Network Architecture}
\begin{figure*}[!t]
    \centering
    \includegraphics[width=7in]{img/X.pdf}
    \caption{From left to right: (a) Letter X in a 7x7 image; (b) Letter X in a 7x7 matrix; (c) A 3x3 convolution kernel; (d) A 5x5 feature map; (e) A 3x3 feature map after pooling; (f) A 3x3 feature map after activating with sigmoid function.}
    \label{X}
\end{figure*}


Neural networks originate from the human perception of the brain. In 1943, American neuroscientists McCulloch and Pitts proposed a theory that every neuron is a multiple-input single-output structure~\cite{mcculloch1943logical}. Furthermore, there are only two possibilities for this output signal: either zero or one, which is very similar to a computer.

In image recognition, if we have a 7x7 image, this 7x7 image has 49 elements. If we write down an `X' in this grid, as shown in Figure~\ref{X}a, in the computer's view, it is a series of numbers. If each cell is either black or white, for example, black is one and white is 0, so what it represents could be a 7x7 matrix. After feeding the program as much data as available, the program will be trained to find parameters to determine if it is an `X' or not. For example, if it is a grey-scale picture, each number is not 0 nor 1, but a grey-scale value from 0 to 255. If it is a color image, then it is RGB colors. Essentially, no matter what the image is, the image can end up replacing itself with a bunch of numbers, and a bunch of numbers can be an input of the neural network. The goal of training is to find the parameters that make the loss function, which measures how far an estimated value is from its true value, smallest. However, using the method described above to train real-world images is time-consuming and computationally expensive. Besides, the algorithm will not recognize it once the image is deflated and rotated or changed.


Based on the Neocognitron Model of Fukushima~\cite{fukushima1982neocognitron}, Yann LeCun invented a practical method for image recognition, called convolutional neural network~\cite{lecun1995convolutional}. The role of convolution is to use a mathematical method to extract these features from the image. The way to extract the features is to use a convolution kernel to do the convolution operation. The convolution kernel is a matrix, usually 3x3 or 5x5. For instance, if we have a convolution kernel that has a 3x3 kernel, and the numbers in it are shown in Figure~\ref{X}c, then a convolution operation will be done with the 7x7 "X" matrix (Figure~\ref{X}b) and the kernel (Figure~\ref{X}c). The operation result of them is also known as a feature map (Figure~\ref{X}d).


The feature map reinforces the features of the convolution kernel, and this convolution kernel (the 3x3 convolution kernel in Figure~\ref{X}c) only has three oblique blocks of pixels being ones. So if the original 7x7 matrix (Figure~\ref{X}b) also has oblique pixel blocks of ones, the number would be extensive when they do the convolution operation. That means we have extracted this feature. The smaller the value of the pixel block in the other positions, the less it satisfies the feature. In a word, with different convolution kernels, we can get different feature maps.

The next step after convolution is pooling. The pooling method can reduce the size of the feature map and maintain similar features to the feature map before pooling pooling the 5x5 matrix (Figure~\ref{X}d) into a relatively small feature map (Figure~\ref{X}e).


The step after pooling is activation. The essence of the activation function is to introduce nonlinear factors to solve problems that a linear model cannot solve.

It is worth noting that the initial convolution kernel may be artificially set. Nevertheless, machine learning will go backwards to adjust and find the most suitable convolution kernel based on its data. Since a picture will generally have many features, there will be many corresponding convolution kernels. After many convolutions and poolings, features, including the slanted lines of the image, the contours, and the colour features, can be found. This information is taken and then feed it into the fully connected network for training, and finally, it is possible to determine what this image is.


\subsection{Convolutional Neural Networks in Image Recognition}
\label{sec2.2}
The above literature review has demonstrated that past literature on carbon inventories of shipping has not focused on small vessels, thus the topic of activity-based emission inventories is a gap in the literature. There is still considerable work to be done to understand how the small boat fleet is being operated, what fuels they are using, and the level of activity for this shipping sector. However, with the development and maturation of a range of computer vision techniques such as convolutional neural networks (CNNs), it may be possible to identify small vessels from open satellite imagery accurately and support the understanding of this ship segment. This study intends to use image recognition algorithms to detect small boats in any sea area, significantly reducing the uncertainty in the estimation of the small boat fleet emission inventories. Besides, it will be in the national interest to account for and mitigate these emissions through effective policies and regulations which incentivizes energy-efficient technologies and scalable zero emission fuels. Further, if countries want to meet their ambitious net-zero carbon emissions targets, they cannot afford to ignore the small boat fleet greenhouse gases emissions.

One of the most fundamental and challenging problems in computer vision is target detection. The main goal of target detection is to determine the location of an object in an image based on a large number of predefined classes. Deep learning techniques, which have emerged in recent years, are a powerful method for learning features directly from data and have led to significant breakthroughs in the field of target detection. Furthermore, with the rise of self-driving cars and face detection, the need for fast and accurate object detection is growing.

In 2012, AlexNet, a deep convolutional neural network (DCNN) proposed by Krizhevsky et al.~\cite{krizhevsky2012imagenet}, achieved record accuracy in image classification at the ImageNet Large-Scale Visual Recognition Challenge (ILSRVC), making convolutional neural networks the dominant paradigm for image recognition. Then, Girshick et al.~\cite{girshick2014rich} introduce Region-Based Convolutional Neural Networks (R-CNN), the first convolutional neural network (CNN)-based object detection method. The R-CNN algorithm represents a two-step approach, where a region proposal is generated firstly, and then a CNN is used for recognition and classification. Compared to the traditional sliding convolutional window to determine the possible regions of objects, R-CNN uses selective search to pre-extract some candidate regions that are more likely to object to avoid computationally costly classification and object search, which makes it faster and significantly less computationally expensive~\cite{ uijlings2013selective, girshick2014rich}. Overall, the R-CNN approach is divided into four steps:

\begin{itemize}
    \item Generate candidate regions.
    \item Extract features using CNN on the candidate regions.
    \item Feed the extracted features into a support vector machine (SVM) classifier.
    \item Correct the object positions by using a regressor.
\end{itemize}

However, R-CNN also has many drawbacks: the selective search method is very slow in generating positive and negative sample candidate regions for the training network, which affects the overall speed of the algorithm; R-CNN needs to perform feature extraction once for each generated candidate region separately, and there are a large number of repeated operations, which limits the algorithm performance ~\cite{huang2017speed}.

Since its inception, R-CNN has undergone several developments and iterations: Fast R-CNN, Faster R-CNN and Mask R-CNN~\cite{girshick2015fast, ren2015faster, he2017mask}. The improvement of Fast R-CNN is the design of a pooling layer structure for ROI (Region of Interest). The pooling stage effectively solves the R-CNN operation that crop and scale image regions to the same size, speeding up the algorithm. Faster R-CNN replaces the selective search method with RPN (Region Proposal Network) ~\cite{ren2015faster}. The selection and judgment of candidate frames are handed over to the RPN for processing, and the candidate regions are subjected to multi-task loss-based classification and localisation processes. 

Several convolutional neural network-based object detection frameworks have recently emerged that can run faster, have a higher detection accuracy, produce cleaner results and are easier to develop. Compared to the Faster RCNN model, the YOLO model can detect better smaller objects, i.e. traffic lights at a distance~\cite{Dwivedi2020YOLOv5}, which is important when detecting objects in satellite images. Also, the YOLO model has a faster end-to-end run-time and detection accuracy than the Faster RCNN~\cite{Dwivedi2020YOLOv5}. Mask R-CNN upgrades the ROI Pooling layer of the Fast R-CNN to an ROI align layer and adds a branching FCN layer, the mask layer, to the bounding box recognition for semantic mask recognition~\cite{he2017mask}. Thus, the Mask R-CNN is essentially an Instance Segmentation algorithm, compared to Semantic Segmentation\footnote{In computer vision, image segmentation is the process of partitioning an image into multiple image segments, also known as image regions or image objects.}. Instance Segmentation is a more fine-grained segmentation of similar objects than Semantic Segmentation.

However, even traditional CNNs can be very useful for large-scale image recognition. Simonyan and Zisserman~\cite{Simonyan2015VeryDC} researched the effect of convolutional network depth on its accuracy in large-scale image recognition setting. Their research found out that even with small (3x3) convolution filters significant accuracy is achieve by pushing the depth to 16 to 19 weight layers.
